#2_курс #Data_Science 

---

```table-of-contents
```

---
# Основные понятия машинного обучения

## I. Моделирование и оценка

### Алгоритм k-means

Визуально мы можем предположить, что здесь [(рис. 1)]() три группы, где точки более близки друг к другу.
![Image Name](Image URL)

Сначала выберем 3 точки на нашей плоскости [(рис. 2)](). Это инициализация, возьмём их рандомно.
![Image Name](Image URL)

Сначала мы разбиваем всё облако так, чтобы они были близки к точкам [(рис. 3)](). Алгоритм относит точки к кластерам по близости к звёздам. Ряд точек оказались расположены близко к границам.
![Image Name](Image URL)

Мы видим, что разбиение не слишком эффективное. Потому мы начинаем перемещать звёзды так, чтобы они встали в центры масс кластеров [(рис. 4, 5)]().
![Image Name](Image URL)

### Ансамбли

Ансамбли и бэггинг – техники, которые используются для повышения точности прогнозирования алгоритмов.

Ансамбли иллюстрирует **теорема Кондросе о жюри присяжных:** если мы возьмём жюри присяжных, их ответы независимы – ответ одного не влияет на выбор другого. Вероятность того, что присяжный примет верное решение, выше $0,5$, т.е. из 100 случаев как минимум в 51% случаев он примет верное решение. Тогда при увеличении количества присяжных мы будем получать 100% верное решение. И наоборот, если вероятность того, что присяжный примет верное решение, ниже $0,5$, то при увеличении количества присяжных вероятность принятия ими верного решения снижается до нуля.

> [!QUESTION]
> Как это применяется в машинном обучении?
> Если мы берём некоторое количество данных, и мы знаем, что один алгоритм ошибается на одном отрезке данных, другой – на втором и т.д., то при смешении результатов всех алгоритмов общая совокупная ошибка начинает падать, за счёт того, что алгоритмы начинают компенсировать друг друга.

>[!EXAMPLE]
>1906 г., Френсис Гальтон сформулировал концепцию мудрости толпы:
> Гальтон выразил свою закономерность, назвав её "мудрость толпы" – даже если каждый не даст правильного ответа, то если мы возьмём мнение большого количества людей и усредним, то можем получить результат, который наиболее точно отражает реальность.

### Бэггинг

У нас есть некий большой датасет – круг на рисунке. Мы можем сделать следующее: в случайном порядке вытаскивать объекты, абсолютно случайно, и разделять их на несколько датасетов.
Это даёт как новые наборы данных, которые по своей структуре, скорее всего, будут напоминать первый набор данных, но при этом будут немного отличаться.
И дальше мы сможем обучать алгоритмы на этих наборах данных, таким образом улучая нашу точность за счёт большого количества данных, которые по своей природе очень похожи на ту данность, которую мы видим в общем наборе.

>[!Note]
>***Бэггинг (баггинг, от англ. bootstrap aggregating)*** – это технология классификации, использующая композиции алгоритмов, каждый из которых обучается независимо. Результат классификации определяется путём голосования.

Бэггинг позволяет снизить процент ошибки классификации в случае, когда высока дисперсия ошибки базового метода.

...
дописать