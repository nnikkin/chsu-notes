#3_курс #Лекция #НейронныеСети 

---
# 1. Персептрон Розенблатта

>[!Info] **Фрэнк Розенблатт** (1928-1971) — известный американский учёный в области психологии, нейрофизиолог ии и искусственного интеллекта (на картинке слева изображён первый нейрокомпьютер «Марк1»)
>![|center](Pasted%20image%2020250310181432.png)

>[!Note] Понятие
><p align="justify"><b>Искусственная нейронная сеть (ИНС)</b> — математическая модель, а также её программное или аппаратное воплощение, построенная по принципу организации и функционирования биологических нейронных сетей — сетей нервных клеток живого организма.</p>

## Персептрон

<p align="justify">Персептрон в первую очередь отвечает за разделимость – он пытается разделить признаки на категории. Категории бывают <i>линейно разделимые</i> и <i>линейно неразделимые</i>:</b>
![[Pasted image 20250310181107.png]]

## Устройство персептрона

![[Pasted image 20250310181153.png]]
![[Pasted image 20250310181219.png]]

## Модель обучения Розенблата – метод коррекции ошибки
<p align="justify">Допустим, мы хотим обучить перцептрон разделять два класса объектов так, чтобы при предъявлении объектов первого класса выход перцептрона был положителен (+1), а при предъявлении объектов второго класса — отрицательным (-1). Для этого выполним следующий алгоритм:</p>
1. Случайным образом выбираем пороги для А-элементов и устанавливаем связи S—А (далее они изменяться не будут).
2. Начальные коэффициенты $W_{i}$ полагаем равными нулю.
3. Предъявляем **обучающую выборку**: объекты (например, круги либо квадраты) с указанием класса, к которым они принадлежат.
4. Показываем перцептрону объект первого класса. При этом некоторые А-элементы возбудятся. Коэффициенты $W_{i}$, соответствующие этим возбуждённым элементам, _увеличиваем_ на 1.
5. Предъявляем объект второго класса и коэффициенты $W_{i}$ тех А-элементов, которые возбудятся при этом показе, _уменьшаем_ на 1.
6. Обе части шага З выполним для всей обучающей выборки. В результате обучения сформируются значения весов связей $W_{i}$.

![[Pasted image 20250310181454.png]]
Наиболее часто используемые функции активации в **методе обратного распространения ошибки**:
$f(s)=\frac{1}{1+e^{-2\alpha s}}$ - экспоненциальная сигмоида
$f(s)=\frac{s}{|s|+\alpha}$ - рациональная сигмоида
$f(s)=th{\frac{s}{\alpha}}=\frac{e^\frac{s}{\alpha}-e^-\frac{s}{\alpha}}{e^\frac{s}{\alpha}+e^-\frac{s}{\alpha}}$ - гиперболический тангенс
где $s$ - выход сумматора нейрона, $\alpha$ - произвольная константа.

![[Pasted image 20250310181609.png]]

в лб4 - если удаляешь строки таблицы - то обосновать